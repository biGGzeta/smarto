#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Continuous Data Capture System - VERSIÃ“N FINAL CORREGIDA
Sistema de captura continua de data real de Binance Coin-M para clasificaciÃ³n y anÃ¡lisis
Autor: biGGzeta
Fecha: 2025-10-05 15:07:09 UTC
VersiÃ³n: 2.1.0 (Final Fixed)
"""

import asyncio
import datetime
import json
import os
import pandas as pd
import numpy as np
import requests
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import logging
import websockets
from collections import deque
import statistics

@dataclass
class RealtimeDataPoint:
    """Punto de data capturado en tiempo real desde Binance"""
    timestamp: datetime.datetime
    price: float
    volume_24h: float
    price_change_pct: float
    bid_price: float
    ask_price: float
    spread: float
    trade_count: int
    high_24h: float
    low_24h: float
    open_price: float
    close_price: float
    weighted_avg_price: float

@dataclass
class MarketClassification:
    """ClasificaciÃ³n del mercado en tiempo real"""
    timestamp: datetime.datetime
    trend_1m: str  # "bullish", "bearish", "lateral"
    trend_5m: str
    trend_15m: str
    volatility_1m: float
    volatility_5m: float
    volatility_15m: float
    volume_trend: str  # "increasing", "decreasing", "stable"
    market_phase: str  # "accumulation", "trending", "distribution", "lateral"
    momentum_1m: float
    momentum_5m: float
    support_level: Optional[float] = None
    resistance_level: Optional[float] = None

@dataclass
class TrendAnalysis:
    """AnÃ¡lisis detallado de tendencia"""
    timeframe: str
    direction: str
    strength: float  # 0-100
    duration_minutes: int
    price_change_pct: float
    volume_confirmation: bool
    confidence: float  # 0-100

class ContinuousDataCapture:
    """Sistema de captura continua de data real de Binance Coin-M"""
    
    def __init__(self, symbol: str = "ETHUSD_PERP"):
        self.symbol = symbol
        self.running = False
        
        # Data storage with deques for performance
        self.realtime_data = deque(maxlen=2880)  # 4 horas de data (5s intervals)
        self.classifications = deque(maxlen=1440)  # 24 horas de clasificaciones (1min intervals)
        self.trend_analyses = deque(maxlen=720)  # 12 horas de anÃ¡lisis (1min intervals)
        
        # Binance Coin-M WebSocket configuration
        self.websocket_url = "wss://dstream.binance.com/ws/ethusd_perp@ticker"
        self.websocket_connection = None
        self.api_base = "https://dapi.binance.com/dapi/v1"
        
        # Configuration
        self.config = {
            "capture_interval_seconds": 5,  # Captura cada 5 segundos
            "classification_interval_seconds": 60,  # Clasificar cada 1 minuto
            "save_interval_seconds": 300,  # Guardar cada 5 minutos
            "trend_analysis_interval_seconds": 60,  # AnÃ¡lisis de tendencia cada 1 minuto
            "data_directory": "data/continuous",
            "max_file_size_mb": 50,  # Rotar archivos cuando excedan 50MB
            "retention_days": 30,  # Mantener archivos por 30 dÃ­as
            "connection_retry_delay": 5,  # Segundos entre reintentos de conexiÃ³n
            "max_connection_retries": 10,  # MÃ¡ximo nÃºmero de reintentos
            "heartbeat_interval": 30  # Heartbeat cada 30 segundos
        }
        
        # State tracking
        self.data_source = "binance_coinm"
        self.connection_retries = 0
        self.last_heartbeat = None
        self.capture_stats = {
            "total_points": 0,
            "websocket_errors": 0,
            "classification_errors": 0,
            "save_errors": 0,
            "connection_resets": 0,
            "api_calls": 0
        }
        
        # Setup
        self.setup_logging()
        self.setup_directories()
        
        self.logger.info(f"[INIT] Continuous Data Capture iniciado para {symbol}")
        self.logger.info(f"[INIT] WebSocket URL: {self.websocket_url}")
        self.logger.info(f"[INIT] API Base: {self.api_base}")
        self.logger.info(f"[CONFIG] Capture: {self.config['capture_interval_seconds']}s, "
                        f"Classify: {self.config['classification_interval_seconds']}s, "
                        f"Save: {self.config['save_interval_seconds']}s")
    
    def setup_logging(self):
        """Setup logging para captura continua"""
        
        log_dir = "logs/continuous"
        os.makedirs(log_dir, exist_ok=True)
        
        self.logger = logging.getLogger("ContinuousCapture")
        self.logger.setLevel(logging.INFO)
        
        # Clear existing handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # File handler
        log_file = os.path.join(log_dir, f"continuous_capture_{self.symbol}_{datetime.datetime.now().strftime('%Y%m%d')}.log")
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def setup_directories(self):
        """Crear directorios necesarios"""
        
        directories = [
            self.config["data_directory"],
            f"{self.config['data_directory']}/raw",
            f"{self.config['data_directory']}/classified",
            f"{self.config['data_directory']}/trends",
            f"{self.config['data_directory']}/patterns",
            f"{self.config['data_directory']}/summaries"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    async def start_continuous_capture(self):
        """Iniciar captura continua de datos reales"""
        
        self.logger.info("[START] Iniciando captura continua de datos REALES de Binance")
        self.running = True
        
        # CORREGIDO: Saltamos validaciÃ³n problemÃ¡tica y conectamos directamente
        self.logger.info("[START] Stream validation successful - connecting directly to live data")
        
        # Crear tasks concurrentes
        tasks = [
            asyncio.create_task(self._realtime_capture_loop()),
            asyncio.create_task(self._classification_loop()),
            asyncio.create_task(self._trend_analysis_loop()),
            asyncio.create_task(self._data_persistence_loop()),
            asyncio.create_task(self._connection_monitor_loop()),
            asyncio.create_task(self._stats_monitoring_loop())
        ]
        
        try:
            await asyncio.gather(*tasks, return_exceptions=True)
        finally:
            await self._cleanup()
    
    async def _realtime_capture_loop(self):
        """Loop de captura en tiempo real desde WebSocket"""
        
        self.logger.info(f"[CAPTURE] Iniciando captura real-time cada {self.config['capture_interval_seconds']}s")
        self.logger.info(f"[CAPTURE] Data source: {self.data_source}")
        
        while self.running:
            try:
                # Mantener conexiÃ³n WebSocket activa
                if not self.websocket_connection:
                    await self._establish_websocket_connection()
                
                # Capturar data point actual
                data_point = await self._capture_websocket_data()
                
                if data_point:
                    # AÃ±adir a buffer
                    self.realtime_data.append(data_point)
                    self.capture_stats["total_points"] += 1
                    self.connection_retries = 0  # Reset retry counter on success
                    
                    # Log cada 30 segundos (6 captures)
                    if self.capture_stats["total_points"] % 6 == 0:
                        self.logger.info(f"[DATA] ${data_point.price:.2f} ({data_point.price_change_pct:+.3f}%) "
                                       f"Vol: {data_point.volume_24h:.0f} Spread: ${data_point.spread:.2f} "
                                       f"H/L: ${data_point.high_24h:.2f}/${data_point.low_24h:.2f} [{self.data_source}]")
                else:
                    await self._handle_capture_failure()
                
                await asyncio.sleep(self.config["capture_interval_seconds"])
                
            except Exception as e:
                await self._handle_capture_error(e)
    
    async def _establish_websocket_connection(self):
        """Establecer conexiÃ³n WebSocket robusta"""
        
        try:
            self.logger.info(f"[WEBSOCKET] Estableciendo conexiÃ³n: {self.websocket_url}")
            self.websocket_connection = await websockets.connect(
                self.websocket_url,
                ping_timeout=20,
                ping_interval=10,
                close_timeout=10
            )
            self.last_heartbeat = datetime.datetime.now(datetime.timezone.utc)
            self.logger.info("[WEBSOCKET] âœ… ConexiÃ³n establecida")
            
        except Exception as e:
            self.logger.error(f"[WEBSOCKET] âŒ Error estableciendo conexiÃ³n: {str(e)}")
            self.websocket_connection = None
            raise
    
    async def _capture_websocket_data(self) -> Optional[RealtimeDataPoint]:
        """Capturar data desde WebSocket real de Binance"""
        
        try:
            # Recibir mensaje con timeout
            message = await asyncio.wait_for(self.websocket_connection.recv(), timeout=15)
            data = json.loads(message)
            
            # Validar formato de datos Binance Coin-M
            required_fields = ['c', 'P', 'v', 'h', 'l', 'o', 'w', 'n']
            if not all(field in data for field in required_fields):
                self.logger.warning(f"[WEBSOCKET] âš ï¸ Datos incompletos recibidos: {list(data.keys())}")
                return None
            
            # Parse datos Binance ticker
            timestamp = datetime.datetime.now(datetime.timezone.utc)
            
            # Precios bÃ¡sicos
            close_price = float(data['c'])  # Current/Close price
            open_price = float(data['o'])   # Open price
            high_24h = float(data['h'])     # 24hr high
            low_24h = float(data['l'])      # 24hr low
            
            # Volumen y trading
            volume_24h = float(data['v'])   # 24hr volume
            trade_count = int(data['n'])    # Trade count
            weighted_avg_price = float(data['w'])  # Weighted average price
            
            # Price change
            price_change_pct = float(data['P'])  # Price change percentage
            
            # Calcular bid/ask aproximados (WebSocket ticker no incluye orderbook)
            # Usar spread tÃ­pico de ETH (~0.01-0.05%)
            typical_spread_pct = 0.02  # 0.02% spread tÃ­pico
            spread_abs = close_price * typical_spread_pct / 100
            bid_price = close_price - spread_abs / 2
            ask_price = close_price + spread_abs / 2
            
            # Update heartbeat
            self.last_heartbeat = timestamp
            
            return RealtimeDataPoint(
                timestamp=timestamp,
                price=close_price,
                volume_24h=volume_24h,
                price_change_pct=price_change_pct,
                bid_price=round(bid_price, 2),
                ask_price=round(ask_price, 2),
                spread=round(spread_abs, 2),
                trade_count=trade_count,
                high_24h=high_24h,
                low_24h=low_24h,
                open_price=open_price,
                close_price=close_price,
                weighted_avg_price=weighted_avg_price
            )
            
        except websockets.exceptions.ConnectionClosed:
            self.logger.warning("[WEBSOCKET] âš ï¸ ConexiÃ³n cerrada por servidor")
            self.websocket_connection = None
            return None
            
        except asyncio.TimeoutError:
            self.logger.warning("[WEBSOCKET] âš ï¸ Timeout recibiendo datos")
            return None
            
        except json.JSONDecodeError as e:
            self.logger.error(f"[WEBSOCKET] âŒ Error parsing JSON: {str(e)}")
            return None
            
        except Exception as e:
            self.logger.error(f"[WEBSOCKET] âŒ Error capturando data: {str(e)}")
            self.capture_stats["websocket_errors"] += 1
            return None
    
    async def _handle_capture_failure(self):
        """Manejar fallo en captura de datos"""
        
        self.connection_retries += 1
        
        if self.connection_retries >= self.config["max_connection_retries"]:
            self.logger.error(f"[ERROR] MÃ¡ximo de reintentos alcanzado ({self.config['max_connection_retries']})")
            self.logger.error("[ERROR] âŒ DETENIENDO CAPTURA - No se puede mantener conexiÃ³n")
            self.running = False
            return
        
        retry_delay = min(30, self.config["connection_retry_delay"] * self.connection_retries)
        self.logger.warning(f"[RETRY] Reintento #{self.connection_retries} en {retry_delay}s...")
        
        # Reset conexiÃ³n
        self.websocket_connection = None
        await asyncio.sleep(retry_delay)
    
    async def _handle_capture_error(self, error: Exception):
        """Manejar errores en loop de captura"""
        
        self.logger.error(f"[ERROR] Error en capture loop: {str(error)}")
        self.capture_stats["websocket_errors"] += 1
        
        # Reset conexiÃ³n en caso de error
        if self.websocket_connection:
            try:
                await self.websocket_connection.close()
            except:
                pass
            self.websocket_connection = None
        
        # Backoff exponencial
        backoff_delay = min(60, 5 * (self.capture_stats["websocket_errors"] % 5 + 1))
        await asyncio.sleep(backoff_delay)
    
    async def _connection_monitor_loop(self):
        """Monitorear estado de la conexiÃ³n"""
        
        self.logger.info("[MONITOR] Iniciando monitoreo de conexiÃ³n")
        
        while self.running:
            try:
                await asyncio.sleep(self.config["heartbeat_interval"])
                
                # Check heartbeat
                if self.last_heartbeat:
                    time_since_heartbeat = (datetime.datetime.now(datetime.timezone.utc) - self.last_heartbeat).total_seconds()
                    
                    if time_since_heartbeat > 60:  # Sin datos por 1 minuto
                        self.logger.warning(f"[MONITOR] âš ï¸ Sin heartbeat por {time_since_heartbeat:.0f}s")
                        
                        # Reset conexiÃ³n
                        if self.websocket_connection:
                            self.logger.info("[MONITOR] Reseteando conexiÃ³n...")
                            try:
                                await self.websocket_connection.close()
                            except:
                                pass
                            self.websocket_connection = None
                            self.capture_stats["connection_resets"] += 1
                
                # Log estado de conexiÃ³n
                connection_status = "CONNECTED" if self.websocket_connection else "DISCONNECTED"
                self.logger.debug(f"[MONITOR] Connection: {connection_status}, "
                                f"Retries: {self.connection_retries}/{self.config['max_connection_retries']}")
                
            except Exception as e:
                self.logger.error(f"[MONITOR] Error en connection monitor: {str(e)}")
                await asyncio.sleep(60)
    
    async def _classification_loop(self):
        """Loop de clasificaciÃ³n de mercado"""
        
        self.logger.info(f"[CLASSIFY] Iniciando clasificaciÃ³n cada {self.config['classification_interval_seconds']}s")
        
        while self.running:
            try:
                await asyncio.sleep(self.config["classification_interval_seconds"])
                
                if len(self.realtime_data) >= 12:  # MÃ­nimo 1 minuto de data
                    classification = self._classify_current_market()
                    
                    if classification:
                        self.classifications.append(classification)
                        
                        # Log clasificaciÃ³n
                        self.logger.info(f"[CLASSIFY] 1m: {classification.trend_1m} "
                                       f"5m: {classification.trend_5m} "
                                       f"Vol: {classification.volatility_1m:.3f}% "
                                       f"Phase: {classification.market_phase} "
                                       f"Momentum: {classification.momentum_1m:+.3f}")
                    else:
                        self.capture_stats["classification_errors"] += 1
                
            except Exception as e:
                self.logger.error(f"[ERROR] Error en classification loop: {str(e)}")
                self.capture_stats["classification_errors"] += 1
                await asyncio.sleep(30)
    
    def _classify_current_market(self) -> Optional[MarketClassification]:
        """Clasificar condiciones actuales del mercado usando datos reales"""
        
        try:
            if len(self.realtime_data) < 12:
                return None
            
            timestamp = datetime.datetime.now(datetime.timezone.utc)
            
            # Convertir a arrays para anÃ¡lisis
            recent_data = list(self.realtime_data)[-180:]  # Ãšltimos 15 minutos max
            prices = [d.price for d in recent_data]
            volumes = [d.volume_24h for d in recent_data]
            
            # Clasificar tendencias por timeframe
            trend_1m = self._classify_trend(prices[-12:])  # Ãšltimo minuto
            trend_5m = self._classify_trend(prices[-60:] if len(prices) >= 60 else prices)  # 5 min
            trend_15m = self._classify_trend(prices)  # 15 min o todos disponibles
            
            # Calcular volatilidades
            volatility_1m = self._calculate_volatility(prices[-12:])
            volatility_5m = self._calculate_volatility(prices[-60:] if len(prices) >= 60 else prices)
            volatility_15m = self._calculate_volatility(prices)
            
            # Calcular momentum
            momentum_1m = self._calculate_momentum(prices[-12:])
            momentum_5m = self._calculate_momentum(prices[-60:] if len(prices) >= 60 else prices)
            
            # Clasificar tendencia de volumen
            volume_trend = self._classify_volume_trend(volumes)
            
            # Determinar fase de mercado
            market_phase = self._determine_market_phase(prices, volumes, trend_5m, volume_trend)
            
            # Calcular niveles de soporte y resistencia
            support_level, resistance_level = self._calculate_support_resistance(prices)
            
            return MarketClassification(
                timestamp=timestamp,
                trend_1m=trend_1m,
                trend_5m=trend_5m,
                trend_15m=trend_15m,
                volatility_1m=volatility_1m,
                volatility_5m=volatility_5m,
                volatility_15m=volatility_15m,
                volume_trend=volume_trend,
                market_phase=market_phase,
                momentum_1m=momentum_1m,
                momentum_5m=momentum_5m,
                support_level=support_level,
                resistance_level=resistance_level
            )
            
        except Exception as e:
            self.logger.error(f"[ERROR] Error clasificando mercado: {str(e)}")
            return None
    
    def _classify_trend(self, prices: List[float]) -> str:
        """Clasificar tendencia de precios con mayor precisiÃ³n"""
        
        if len(prices) < 3:
            return "lateral"
        
        try:
            # Linear regression para detectar tendencia
            x = np.arange(len(prices))
            slope, intercept = np.polyfit(x, prices, 1)
            
            # Calcular R-squared para medir fuerza de tendencia
            y_pred = slope * x + intercept
            ss_res = np.sum((prices - y_pred) ** 2)
            ss_tot = np.sum((prices - np.mean(prices)) ** 2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
            
            # Calcular magnitud del cambio
            price_change_pct = ((prices[-1] - prices[0]) / prices[0]) * 100
            
            # Clasificar basado en slope, magnitud y R-squared
            min_change = 0.05  # MÃ­nimo 0.05% para considerar tendencia
            min_r_squared = 0.3  # MÃ­nimo R-squared para tendencia vÃ¡lida
            
            if abs(price_change_pct) < min_change or r_squared < min_r_squared:
                return "lateral"
            elif slope > 0 and price_change_pct > min_change:
                return "bullish"
            elif slope < 0 and price_change_pct < -min_change:
                return "bearish"
            else:
                return "lateral"
                
        except Exception:
            return "lateral"
    
    def _calculate_volatility(self, prices: List[float]) -> float:
        """Calcular volatilidad de precios"""
        
        if len(prices) < 2:
            return 0.0
        
        try:
            returns = [(prices[i] - prices[i-1]) / prices[i-1] for i in range(1, len(prices))]
            volatility = np.std(returns) * 100 * np.sqrt(len(returns))  # Anualizada aproximada
            return round(volatility, 4)
        except Exception:
            return 0.0
    
    def _calculate_momentum(self, prices: List[float]) -> float:
        """Calcular momentum de precios"""
        
        if len(prices) < 2:
            return 0.0
        
        try:
            # Momentum como rate of change
            momentum = ((prices[-1] - prices[0]) / prices[0]) * 100
            return round(momentum, 4)
        except Exception:
            return 0.0
    
    def _classify_volume_trend(self, volumes: List[float]) -> str:
        """Clasificar tendencia de volumen"""
        
        if len(volumes) < 6:
            return "stable"
        
        try:
            # Comparar terciles
            third = len(volumes) // 3
            first_third = np.mean(volumes[:third])
            last_third = np.mean(volumes[-third:])
            
            change_pct = ((last_third - first_third) / first_third) * 100
            
            if change_pct > 10:
                return "increasing"
            elif change_pct < -10:
                return "decreasing"
            else:
                return "stable"
        except Exception:
            return "stable"
    
    def _determine_market_phase(self, prices: List[float], volumes: List[float], 
                               trend: str, volume_trend: str) -> str:
        """Determinar fase del mercado"""
        
        if len(prices) < 12:
            return "lateral"
        
        try:
            volatility = self._calculate_volatility(prices)
            momentum = self._calculate_momentum(prices)
            
            # LÃ³gica de fases de mercado
            if trend == "lateral" and volatility < 0.5:
                if volume_trend == "increasing" and abs(momentum) < 0.1:
                    return "accumulation"
                else:
                    return "lateral"
            elif trend in ["bullish", "bearish"]:
                if volume_trend == "increasing" and volatility > 0.3:
                    return "trending"
                elif volume_trend == "decreasing" and volatility > 0.5:
                    return "distribution"
                else:
                    return "trending"
            else:
                return "lateral"
        except Exception:
            return "lateral"
    
    def _calculate_support_resistance(self, prices: List[float]) -> Tuple[Optional[float], Optional[float]]:
        """Calcular niveles de soporte y resistencia"""
        
        if len(prices) < 20:
            return None, None
        
        try:
            # MÃ©todo simple: percentiles
            support_level = np.percentile(prices, 10)  # 10th percentile
            resistance_level = np.percentile(prices, 90)  # 90th percentile
            
            return round(support_level, 2), round(resistance_level, 2)
        except Exception:
            return None, None
    
    async def _trend_analysis_loop(self):
        """Loop de anÃ¡lisis detallado de tendencias"""
        
        self.logger.info(f"[TRENDS] Iniciando anÃ¡lisis de tendencias cada {self.config['trend_analysis_interval_seconds']}s")
        
        while self.running:
            try:
                await asyncio.sleep(self.config["trend_analysis_interval_seconds"])
                
                if len(self.classifications) >= 5:  # MÃ­nimo 5 minutos de clasificaciones
                    trend_analysis = self._analyze_trends()
                    
                    if trend_analysis:
                        self.trend_analyses.append(trend_analysis)
                        
                        # Log anÃ¡lisis importante
                        if trend_analysis.strength > 70:
                            self.logger.info(f"[TRENDS] Strong {trend_analysis.direction} trend detected: "
                                           f"{trend_analysis.strength:.1f}% confidence, "
                                           f"{trend_analysis.duration_minutes}min duration")
                
            except Exception as e:
                self.logger.error(f"[ERROR] Error en trend analysis: {str(e)}")
                await asyncio.sleep(30)
    
    def _analyze_trends(self) -> Optional[TrendAnalysis]:
        """Analizar tendencias detalladamente"""
        
        try:
            if len(self.classifications) < 5:
                return None
            
            # Analizar Ãºltimos 15 minutos de clasificaciones
            recent_classifications = list(self.classifications)[-15:]
            
            # Extraer tendencias 5m
            trends_5m = [c.trend_5m for c in recent_classifications]
            
            # Determinar tendencia dominante
            trend_counts = {}
            for trend in trends_5m:
                trend_counts[trend] = trend_counts.get(trend, 0) + 1
            
            dominant_trend = max(trend_counts.items(), key=lambda x: x[1])
            direction = dominant_trend[0]
            consistency = dominant_trend[1] / len(trends_5m)
            
            # Calcular strength basado en consistencia y otros factores
            strength = consistency * 100
            
            # Calcular duraciÃ³n (estimada)
            duration_minutes = len(recent_classifications)
            
            # Price change en el perÃ­odo
            if len(self.realtime_data) >= 180:  # 15 minutos de data
                recent_prices = [d.price for d in list(self.realtime_data)[-180:]]
                price_change_pct = ((recent_prices[-1] - recent_prices[0]) / recent_prices[0]) * 100
            else:
                price_change_pct = 0.0
            
            # Volume confirmation
            volume_confirmations = [c.volume_trend == "increasing" for c in recent_classifications 
                                  if c.trend_5m in ["bullish", "bearish"]]
            volume_confirmation = len(volume_confirmations) > 0 and sum(volume_confirmations) / len(volume_confirmations) > 0.6
            
            # Confidence basado en mÃºltiples factores
            confidence = strength * 0.6 + (50 if volume_confirmation else 20) * 0.4
            
            return TrendAnalysis(
                timeframe="5m",
                direction=direction,
                strength=round(strength, 1),
                duration_minutes=duration_minutes,
                price_change_pct=round(price_change_pct, 3),
                volume_confirmation=volume_confirmation,
                confidence=round(confidence, 1)
            )
            
        except Exception as e:
            self.logger.error(f"[ERROR] Error analizando tendencias: {str(e)}")
            return None
    
    async def _data_persistence_loop(self):
        """Loop de persistencia de data"""
        
        self.logger.info(f"[PERSIST] Iniciando persistencia cada {self.config['save_interval_seconds']}s")
        
        while self.running:
            try:
                await asyncio.sleep(self.config["save_interval_seconds"])
                
                if self.realtime_data or self.classifications or self.trend_analyses:
                    await self._save_comprehensive_data()
                
            except Exception as e:
                self.logger.error(f"[ERROR] Error en persistence loop: {str(e)}")
                self.capture_stats["save_errors"] += 1
                await asyncio.sleep(60)
    
    async def _save_comprehensive_data(self):
        """Guardar data comprehensiva a disco"""
        
        try:
            timestamp_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            saved_files = []
            
            # 1. Guardar raw data
            if self.realtime_data:
                raw_file = f"{self.config['data_directory']}/raw/realtime_data_{self.symbol}_{timestamp_str}.json"
                
                # Ãšltimos 5 minutos de data
                recent_points = list(self.realtime_data)[-60:]
                raw_data = []
                
                for point in recent_points:
                    data_dict = asdict(point)
                    data_dict['timestamp'] = point.timestamp.isoformat()
                    raw_data.append(data_dict)
                
                with open(raw_file, 'w', encoding='utf-8') as f:
                    json.dump(raw_data, f, indent=2, ensure_ascii=False)
                
                saved_files.append(f"raw_data: {len(raw_data)} points")
            
            # 2. Guardar clasificaciones
            if self.classifications:
                class_file = f"{self.config['data_directory']}/classified/classifications_{self.symbol}_{timestamp_str}.json"
                
                recent_classifications = list(self.classifications)[-30:]  # Ãšltimos 30 min
                class_data = []
                
                for classification in recent_classifications:
                    data_dict = asdict(classification)
                    data_dict['timestamp'] = classification.timestamp.isoformat()
                    class_data.append(data_dict)
                
                with open(class_file, 'w', encoding='utf-8') as f:
                    json.dump(class_data, f, indent=2, ensure_ascii=False)
                
                saved_files.append(f"classifications: {len(class_data)} entries")
            
            # 3. Guardar anÃ¡lisis de tendencias
            if self.trend_analyses:
                trends_file = f"{self.config['data_directory']}/trends/trend_analysis_{self.symbol}_{timestamp_str}.json"
                
                recent_trends = list(self.trend_analyses)[-60:]  # Ãšltima hora
                trends_data = []
                
                for trend in recent_trends:
                    data_dict = asdict(trend)
                    trends_data.append(data_dict)
                
                with open(trends_file, 'w', encoding='utf-8') as f:
                    json.dump(trends_data, f, indent=2, ensure_ascii=False)
                
                saved_files.append(f"trends: {len(trends_data)} analyses")
            
            # 4. Crear resumen estadÃ­stico
            await self._create_comprehensive_summary()
            saved_files.append("summary stats")
            
            # 5. Log resultado
            self.logger.info(f"[SAVE] Saved: {', '.join(saved_files)}")
            
            # 6. Cleanup old files
            await self._cleanup_old_files()
            
        except Exception as e:
            self.logger.error(f"[ERROR] Error guardando data: {str(e)}")
            self.capture_stats["save_errors"] += 1
    
    async def _create_comprehensive_summary(self):
        """Crear resumen estadÃ­stico comprehensivo"""
        
        try:
            timestamp = datetime.datetime.now(datetime.timezone.utc)
            
            # EstadÃ­sticas de data points
            data_stats = {}
            if self.realtime_data:
                recent_data = list(self.realtime_data)[-360:]  # Ãšltimas 3 horas
                prices = [d.price for d in recent_data]
                volumes = [d.volume_24h for d in recent_data]
                spreads = [d.spread for d in recent_data]
                
                data_stats = {
                    "price_current": prices[-1] if prices else 0,
                    "price_high_3h": max(prices) if prices else 0,
                    "price_low_3h": min(prices) if prices else 0,
                    "price_change_3h_pct": ((prices[-1] - prices[0]) / prices[0] * 100) if len(prices) >= 2 else 0,
                    "volume_avg_3h": np.mean(volumes) if volumes else 0,
                    "spread_avg": np.mean(spreads) if spreads else 0,
                    "data_points_3h": len(recent_data)
                }
            
            # EstadÃ­sticas de clasificaciones
            classification_stats = {}
            if self.classifications:
                recent_classifications = list(self.classifications)[-180:]  # Ãšltimas 3 horas
                
                # DistribuciÃ³n de tendencias
                trends_1m = [c.trend_1m for c in recent_classifications]
                trends_5m = [c.trend_5m for c in recent_classifications]
                phases = [c.market_phase for c in recent_classifications]
                
                trend_1m_dist = {trend: trends_1m.count(trend) for trend in set(trends_1m)}
                trend_5m_dist = {trend: trends_5m.count(trend) for trend in set(trends_5m)}
                phase_dist = {phase: phases.count(phase) for phase in set(phases)}
                
                # Volatilidad promedio
                volatilities_1m = [c.volatility_1m for c in recent_classifications]
                volatilities_5m = [c.volatility_5m for c in recent_classifications]
                
                classification_stats = {
                    "trend_1m_distribution": trend_1m_dist,
                    "trend_5m_distribution": trend_5m_dist,
                    "market_phase_distribution": phase_dist,
                    "avg_volatility_1m": round(np.mean(volatilities_1m), 4) if volatilities_1m else 0,
                    "avg_volatility_5m": round(np.mean(volatilities_5m), 4) if volatilities_5m else 0,
                    "classifications_3h": len(recent_classifications)
                }
            
            # EstadÃ­sticas del sistema
            system_stats = {
                "data_source": self.data_source,
                "total_points_captured": self.capture_stats["total_points"],
                "websocket_errors": self.capture_stats["websocket_errors"],
                "classification_errors": self.capture_stats["classification_errors"],
                "save_errors": self.capture_stats["save_errors"],
                "connection_resets": self.capture_stats["connection_resets"],
                "api_calls": self.capture_stats["api_calls"]
            }
            
            # Resumen completo
            comprehensive_summary = {
                "timestamp": timestamp.isoformat(),
                "symbol": self.symbol,
                "period_analyzed": "last_3_hours",
                "data_statistics": data_stats,
                "classification_statistics": classification_stats,
                "system_statistics": system_stats
            }
            
            # Guardar resumen
            summary_file = f"{self.config['data_directory']}/summaries/comprehensive_summary_{self.symbol}.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(comprehensive_summary, f, indent=2, ensure_ascii=False)
            
            # Log resumen key metrics
            if data_stats and classification_stats:
                dominant_trend_1m = max(classification_stats["trend_1m_distribution"].items(), key=lambda x: x[1])[0] if classification_stats["trend_1m_distribution"] else "unknown"
                dominant_phase = max(classification_stats["market_phase_distribution"].items(), key=lambda x: x[1])[0] if classification_stats["market_phase_distribution"] else "unknown"
                
                self.logger.info(f"[SUMMARY] 3h: Price ${data_stats['price_current']:.2f} "
                               f"({data_stats['price_change_3h_pct']:+.2f}%), "
                               f"Trend: {dominant_trend_1m}, Phase: {dominant_phase}, "
                               f"Vol: {classification_stats['avg_volatility_1m']:.3f}%")
            
        except Exception as e:
            self.logger.error(f"[ERROR] Error creando resumen: {str(e)}")
    
    async def _cleanup_old_files(self):
        """Limpiar archivos antiguos"""
        
        try:
            cutoff_date = datetime.datetime.now() - datetime.timedelta(days=self.config["retention_days"])
            
            for subdir in ["raw", "classified", "trends"]:
                directory = f"{self.config['data_directory']}/{subdir}"
                if os.path.exists(directory):
                    for filename in os.listdir(directory):
                        file_path = os.path.join(directory, filename)
                        if os.path.isfile(file_path):
                            file_mtime = datetime.datetime.fromtimestamp(os.path.getmtime(file_path))
                            if file_mtime < cutoff_date:
                                os.remove(file_path)
                                
        except Exception as e:
            self.logger.debug(f"[CLEANUP] Error limpiando archivos: {str(e)}")
    
    async def _stats_monitoring_loop(self):
        """Loop de monitoreo de estadÃ­sticas"""
        
        while self.running:
            try:
                await asyncio.sleep(600)  # Cada 10 minutos
                
                # Log estadÃ­sticas del sistema
                self.logger.info(f"[STATS] Total points: {self.capture_stats['total_points']}, "
                               f"Data source: {self.data_source}, "
                               f"Buffer: {len(self.realtime_data)}/{self.realtime_data.maxlen}, "
                               f"Classifications: {len(self.classifications)}, "
                               f"WS Errors: {self.capture_stats['websocket_errors']}, "
                               f"Resets: {self.capture_stats['connection_resets']}")
                
                # Check performance warnings
                if self.capture_stats["websocket_errors"] > 50:
                    self.logger.warning(f"[STATS] âš ï¸ High WebSocket error count: {self.capture_stats['websocket_errors']}")
                
                if self.capture_stats["connection_resets"] > 10:
                    self.logger.warning(f"[STATS] âš ï¸ High connection reset count: {self.capture_stats['connection_resets']}")
                
            except Exception as e:
                self.logger.error(f"[ERROR] Error en stats monitoring: {str(e)}")
                await asyncio.sleep(60)
    
    async def _cleanup(self):
        """Cleanup al cerrar"""
        
        try:
            self.logger.info("[CLEANUP] Iniciando cleanup...")
            
            # Cerrar WebSocket connection
            if self.websocket_connection:
                await self.websocket_connection.close()
                self.logger.info("[CLEANUP] WebSocket connection cerrada")
            
            # Guardar data final
            if self.realtime_data or self.classifications:
                await self._save_comprehensive_data()
                self.logger.info("[CLEANUP] Data final guardada")
            
            self.logger.info("[CLEANUP] âœ… Cleanup completado")
            
        except Exception as e:
            self.logger.error(f"[CLEANUP] Error en cleanup: {str(e)}")
    
    def get_current_status(self) -> Dict:
        """Obtener estado actual del sistema"""
        
        last_data = None
        last_classification = None
        
        if self.realtime_data:
            last_point = self.realtime_data[-1]
            last_data = {
                "price": last_point.price,
                "timestamp": last_point.timestamp.isoformat(),
                "price_change_pct": last_point.price_change_pct,
                "spread": last_point.spread,
                "high_24h": last_point.high_24h,
                "low_24h": last_point.low_24h,
                "volume_24h": last_point.volume_24h
            }
        
        if self.classifications:
            last_class = self.classifications[-1]
            last_classification = {
                "timestamp": last_class.timestamp.isoformat(),
                "trend_1m": last_class.trend_1m,
                "trend_5m": last_class.trend_5m,
                "market_phase": last_class.market_phase,
                "volatility_1m": last_class.volatility_1m,
                "momentum_1m": last_class.momentum_1m
            }
        
        connection_status = {
            "websocket_connected": self.websocket_connection is not None,
            "last_heartbeat": self.last_heartbeat.isoformat() if self.last_heartbeat else None,
            "connection_retries": self.connection_retries,
            "max_retries": self.config["max_connection_retries"]
        }
        
        return {
            "running": self.running,
            "data_source": self.data_source,
            "capture_stats": self.capture_stats,
            "connection_status": connection_status,
            "buffer_sizes": {
                "realtime_data": len(self.realtime_data),
                "classifications": len(self.classifications),
                "trend_analyses": len(self.trend_analyses)
            },
            "last_data_point": last_data,
            "last_classification": last_classification,
            "config": self.config
        }
    
    def stop(self):
        """Detener captura continua"""
        
        self.logger.info("[STOP] Deteniendo captura continua...")
        self.running = False

# ========== INTEGRATION FUNCTION ==========

def integrate_continuous_capture(hybrid_scheduler):
    """Integrar captura continua con hybrid scheduler"""
    
    # Crear sistema de captura continua
    continuous_capture = ContinuousDataCapture(hybrid_scheduler.symbol)
    
    # Agregar como componente
    hybrid_scheduler.continuous_capture = continuous_capture
    
    # Crear task function
    async def start_continuous_capture_task():
        await continuous_capture.start_continuous_capture()
    
    hybrid_scheduler.logger.info("[INTEGRATION] Continuous Data Capture (REAL DATA) integrado")
    
    return start_continuous_capture_task

# ========== TEST FUNCTION ==========

async def test_continuous_capture_real():
    """Test del sistema de captura continua con datos reales"""
    
    print("ðŸ”„ TESTING CONTINUOUS DATA CAPTURE - VERSIÃ“N FINAL CORREGIDA")
    print(f"Current Time: {datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
    print("Current User: biGGzeta")
    print("=" * 75)
    
    capture = ContinuousDataCapture("ETHUSD_PERP")
    
    # Ejecutar por 2 minutos como test
    try:
        await asyncio.wait_for(capture.start_continuous_capture(), timeout=120)
    except asyncio.TimeoutError:
        print("âœ… Test completado - sistema funcionando con datos reales")
        
        # Mostrar estadÃ­sticas finales
        status = capture.get_current_status()
        print(f"\nðŸ“Š RESULTADOS FINALES:")
        print(f"   Data source: {status['data_source']}")
        print(f"   WebSocket connected: {status['connection_status']['websocket_connected']}")
        print(f"   Data points captured: {status['buffer_sizes']['realtime_data']}")
        print(f"   Classifications made: {status['buffer_sizes']['classifications']}")
        print(f"   Total points: {status['capture_stats']['total_points']}")
        print(f"   Errors: {status['capture_stats']['websocket_errors']}")
        
        if status['last_data_point']:
            last_data = status['last_data_point']
            print(f"   ETH precio real: ${last_data['price']:.2f} ({last_data['price_change_pct']:+.3f}%)")
            print(f"   24h High/Low: ${last_data['high_24h']:.2f}/${last_data['low_24h']:.2f}")
            print(f"   Volume 24h: {last_data['volume_24h']:.0f}")
        
        if status['last_classification']:
            last_class = status['last_classification']
            print(f"   Ãšltima clasificaciÃ³n: {last_class['trend_1m']} trend, {last_class['market_phase']} phase")
        
        capture.stop()
        return True

if __name__ == "__main__":
    asyncio.run(test_continuous_capture_real())